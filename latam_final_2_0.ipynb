{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_LwSgxxyKQt0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mespealva/numpy/blob/main/latam_final_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üìå EXTRACCI√ìN"
      ],
      "metadata": {
        "id": "_LwSgxxyKQt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "uV2tSTZRJ23P",
        "outputId": "f914073b-3e56-4a02-8fd8-aede73b3bdac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/df_limpo.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4233134006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/df_limpo.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/df_limpo.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('/content/df_limpo.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver las columnas disponibles\n",
        "df.columns"
      ],
      "metadata": {
        "id": "yF8y8a9nKYC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verificar la estructura general\n",
        "df.info()"
      ],
      "metadata": {
        "id": "0PqacHw8VbD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üõ†Ô∏è Preparaci√≥n de los Datos\n"
      ],
      "metadata": {
        "id": "oYzF90qOKi8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ‚úîÔ∏è Remover columnas irrelevantes"
      ],
      "metadata": {
        "id": "1flBm3_5Knlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la etapa de eliminaci√≥n de columnas irrelevantes, el objetivo es excluir variables que:\n",
        "\n",
        "* No tienen valor predictivo (ej.: identificadores √∫nicos).\n",
        "* Son redundantes con otras.\n",
        "* Pueden causar fuga de datos (*data leakage*).\n"
      ],
      "metadata": {
        "id": "jf2vFh8bKrGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['customerID'])"
      ],
      "metadata": {
        "id": "VFch7ExcKl4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ‚úîÔ∏è Agrupaci√≥n de No y No service"
      ],
      "metadata": {
        "id": "EvBIhN2BLRfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D3UhvhGdX4zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Resumen: Por qu√© agrupamos `\"No\"` y `\"No internet service\"`**\n",
        "\n",
        "üéØ **Objetivo**\n",
        "\n",
        "Reducir **multicolinealidad** y **simplificar los datos** sin perder relevancia para el modelo predictivo.\n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è **El problema original**\n",
        "\n",
        "* Variables como `OnlineSecurity`, `StreamingTV`, etc., ten√≠an **tres categor√≠as**:\n",
        "\n",
        "  * `\"Yes\"` ‚Üí cliente usa el servicio\n",
        "  * `\"No\"` ‚Üí cliente tiene internet, pero no contrat√≥ el servicio\n",
        "  * `\"No internet service\"` ‚Üí cliente **ni siquiera tiene internet**, por lo tanto no puede usar el servicio\n",
        "\n",
        "* Esto generaba **multicolinealidad perfecta** al transformar estas categor√≠as en *dummies*, lo que:\n",
        "\n",
        "  * Creaba **correlaci√≥n 1.0** entre variables\n",
        "  * Generaba **VIF infinito**\n",
        "  * Compromet√≠a la estabilidad y el rendimiento de los modelos\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **La soluci√≥n: agrupar `\"No internet service\"` como `\"No\"`**\n",
        "\n",
        "* **Agrupamos** `\"No internet service\"` como `\"No\"` para simplificar la variable:\n",
        "\n",
        "  * Ahora: `\"Yes\"` = usa el servicio\n",
        "    `\"No\"` = no usa el servicio (por cualquier motivo)\n",
        "\n",
        "* Esto **reduce la dimensionalidad** y **evita multicolinealidad**.\n",
        "\n",
        "* La informaci√≥n de que el cliente **no tiene internet** sigue estando en la variable `InternetService`.\n",
        "\n",
        "---\n",
        "\n",
        "üß† **¬øY el impacto?**\n",
        "\n",
        "* Perdemos un matiz (por qu√© el cliente no usa el servicio), **pero**:\n",
        "\n",
        "  * Esto rara vez afecta el rendimiento del modelo\n",
        "  * Ganamos **m√°s robustez, menos ruido y menos redundancia**\n"
      ],
      "metadata": {
        "id": "fJWNH4XYLHKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# para crear uno nuevo\n",
        "df_clean = df.copy()\n",
        "\n",
        "# === Etapa 1: Agrupar \"No internet service\" como \"No\"\n",
        "cols_to_fix = [\n",
        "    'internet.OnlineSecurity', 'internet.OnlineBackup', 'internet.DeviceProtection',\n",
        "    'internet.TechSupport', 'internet.StreamingTV', 'internet.StreamingMovies'\n",
        "]\n",
        "\n",
        "for col in cols_to_fix:\n",
        "    df_clean[col] = df_clean[col].replace('No internet service', 'No')\n",
        "\n",
        "# === Etapa 2: One-hot encoding (sin dummy trap)\n",
        "categorical_cols = [\n",
        "       'Churn', 'customer.gender', 'customer.Partner', 'customer.Dependents',\n",
        "       'phone.PhoneService', 'phone.MultipleLines', 'internet.InternetService',\n",
        "       'internet.OnlineSecurity', 'internet.OnlineBackup',\n",
        "       'internet.DeviceProtection', 'internet.TechSupport',\n",
        "       'internet.StreamingTV', 'internet.StreamingMovies', 'account.Contract',\n",
        "       'account.PaperlessBilling', 'account.PaymentMethod'\n",
        "]\n",
        "\n",
        "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# ¬°Listo para usar!\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "ULV3-k15Kbr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.info()"
      ],
      "metadata": {
        "id": "bexOAqNKN9ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úîÔ∏è Verificaci√≥n nuevamente de los valores nulos\n"
      ],
      "metadata": {
        "id": "5QuWwyhCMwVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.isnull().sum()"
      ],
      "metadata": {
        "id": "uZr4qPDzMF9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica valores nulos en las dos columnas\n",
        "print(df_encoded[['Total.Day', 'account.Charges.Total']].isnull().sum())"
      ],
      "metadata": {
        "id": "AAQ5vjzgMyh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elimina filas con valores nulos en las columnas especificadas\n",
        "df_encoded = df_encoded.dropna(subset=['Total.Day', 'account.Charges.Total'])"
      ],
      "metadata": {
        "id": "-rgWjEPeM197"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tratamiento de valores nulos**\n",
        "\n",
        "Al identificar valores nulos en las columnas `Total.Day` y `account.Charges.Total`, es necesario decidir entre **eliminar** o **rellenar** esas entradas.\n",
        "\n",
        "* **Eliminar filas**: como el n√∫mero de valores nulos es peque√±o (11 filas en m√°s de 7 mil), podemos eliminar esas filas sin afectar el an√°lisis. Esto evita introducir distorsiones en los resultados.\n",
        "\n",
        "* **Reemplazar por cero**: esta opci√≥n puede usarse cuando el valor nulo representa ausencia de dato o servicio (por ejemplo, ninguna cobranza), pero puede distorsionar promedios y sumas si no es el caso real.\n",
        "\n",
        "En este proyecto, optamos por **eliminar las filas con valores nulos** por seguridad y simplicidad, asegurando que los datos usados est√©n completos.\n"
      ],
      "metadata": {
        "id": "eyzl1df-M8h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.isnull().sum()"
      ],
      "metadata": {
        "id": "-myQyFahM36j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úîÔ∏è Normalizaci√≥n/Estandarizaci√≥n\n"
      ],
      "metadata": {
        "id": "SUJZyPe-NG0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÑ Normalizaci√≥n de los datos\n",
        "\n",
        "La normalizaci√≥n es un paso com√∫n en el preprocesamiento de datos, especialmente importante para algoritmos que son **sensibles a la escala de los atributos**, como:\n",
        "\n",
        "* KNN (K-Nearest Neighbors)\n",
        "* Redes Neuronales\n",
        "* Regresi√≥n Log√≠stica\n",
        "* SVM (Support Vector Machine)\n",
        "\n",
        "Modelos basados en √°rboles (como Decision Tree, Random Forest y XGBoost) **no requieren normalizaci√≥n**, ya que no dependen de la escala de los datos para construir sus reglas de decisi√≥n.\n",
        "\n",
        "En este proyecto, aplicaremos la **normalizaci√≥n Min-Max**, que transforma los valores al rango **\\[0, 1]**. Esto ayuda a garantizar que todas las variables num√©ricas contribuyan de forma equilibrada al modelo.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Seleccionando solo columnas num√©ricas (excepto la variable target, si ya est√° separada)\n",
        "colunas_numericas = dados.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Inicializando el scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Aplicando la normalizaci√≥n\n",
        "dados[colunas_numericas] = scaler.fit_transform(dados[colunas_numericas])\n",
        "\n",
        "# Mostrando los datos normalizados\n",
        "dados.head()\n",
        "```\n"
      ],
      "metadata": {
        "id": "M75u2sqENL3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚â° Correlaci√≥n entre las variables\n"
      ],
      "metadata": {
        "id": "fx5QNXPcNQSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df_encoded.corr()"
      ],
      "metadata": {
        "id": "iZfaKxAINWKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Import the numpy library\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20,16))\n",
        "ax = sns.heatmap(np.round(corr, 2), vmax=1, vmin=-1, center=0,\n",
        "            square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .5})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5rydp0JgOM9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Definir la variable objetivo\n",
        "target_var = 'Churn_Yes'\n",
        "\n",
        "# Definir el umbral m√≠nimo de correlaci√≥n absoluta para selecci√≥n\n",
        "limiar = 0.2\n",
        "\n",
        "# Filtrar variables que tengan correlaci√≥n absoluta >= umbral con la variable objetivo\n",
        "# Esto crea una lista con las variables relevantes\n",
        "variaveis_relevantes = corr.index[abs(corr[target_var]) >= limiar].tolist()\n",
        "\n",
        "# Asegurar que la variable objetivo est√© en la lista (si no est√°, a√±adirla)\n",
        "if target_var not in variaveis_relevantes:\n",
        "    variaveis_relevantes.append(target_var)\n",
        "\n",
        "# Crear una matriz de correlaci√≥n solo con las variables seleccionadas\n",
        "corr_filtrada = corr.loc[variaveis_relevantes, variaveis_relevantes]\n",
        "\n",
        "# Generar una m√°scara para ocultar el tri√°ngulo superior de la matriz (incluida la diagonal)\n",
        "mascara = np.triu(np.ones_like(corr_filtrada, dtype=bool))\n",
        "\n",
        "# Graficar el heatmap con la m√°scara aplicada para mejor visualizaci√≥n\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(\n",
        "    corr_filtrada,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap='coolwarm',\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={\"shrink\": 0.7},\n",
        "    mask=mascara\n",
        ")\n",
        "plt.title(f'Heatmap de variables con correlaci√≥n >= {limiar} con \"{target_var}\"')\n",
        "plt.show()\n",
        "```\n"
      ],
      "metadata": {
        "id": "CUAEwy-GOXCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicaci√≥n del uso del umbral y de la m√°scara en el heatmap de correlaci√≥n**\n",
        "\n",
        "Cuando trabajamos con an√°lisis de correlaci√≥n entre muchas variables, la matriz de correlaci√≥n puede volverse muy grande y dif√≠cil de interpretar visualmente. Por ejemplo, si tenemos 50 variables, la matriz tendr√° 50 x 50 = 2500 valores, lo que genera un gr√°fico confuso y poco informativo.\n",
        "\n",
        "**Uso del umbral para selecci√≥n de variables relevantes**\n",
        "\n",
        "Para facilitar el an√°lisis, elegimos un **umbral de correlaci√≥n absoluta** respecto a la variable objetivo (en este caso, `\"Evasi√≥n\"`).\n",
        "\n",
        "* Este umbral es un valor m√≠nimo para considerar que la correlaci√≥n es relevante o significativa para nuestro an√°lisis.\n",
        "* Por ejemplo, un umbral de 0.3 significa que solo vamos a mirar variables cuya correlaci√≥n con `\"Evasi√≥n\"` sea mayor que 0.3 (positiva o negativa).\n",
        "* Variables con correlaci√≥n por debajo de ese valor tienden a no tener influencia importante o clara sobre la variable objetivo y, por eso, se descartan para esta visualizaci√≥n.\n",
        "* Esto ayuda a **reducir el n√∫mero de variables**, haciendo el heatmap m√°s legible y enfocado en las relaciones importantes.\n",
        "\n",
        "> **Nota:** El valor exacto del umbral puede variar seg√∫n el contexto, pero generalmente valores entre 0.2 y 0.5 son buenos puntos de partida para an√°lisis exploratorios.\n",
        "\n",
        "**Uso de la m√°scara del tri√°ngulo superior (tri√°ngulo invertido)**\n",
        "\n",
        "La matriz de correlaci√≥n es **sim√©trica** respecto a la diagonal principal:\n",
        "\n",
        "* El valor en la posici√≥n `(i, j)` es igual al valor en la posici√≥n `(j, i)`.\n",
        "* Esto significa que el heatmap muestra informaci√≥n repetida en el tri√°ngulo superior e inferior de la matriz.\n",
        "\n",
        "Para mejorar la claridad del gr√°fico, aplicamos una **m√°scara para ocultar el tri√°ngulo superior (incluyendo la diagonal)**, dejando visible solo el tri√°ngulo inferior.\n",
        "\n",
        "Esto trae las siguientes ventajas:\n",
        "\n",
        "* **Evita redundancia visual**, mostrando cada par de variables una √∫nica vez.\n",
        "* **Hace el gr√°fico m√°s limpio y f√°cil de interpretar.**\n",
        "* Ayuda a destacar las correlaciones importantes sin confusi√≥n.\n",
        "\n",
        "**¬øPor qu√© esta aproximaci√≥n es adecuada para nuestro problema?**\n",
        "\n",
        "* Nuestro foco es entender qu√© variables tienen mayor correlaci√≥n con la variable objetivo `\"Evasi√≥n\"`.\n",
        "* Filtrando por las variables m√°s relevantes, obtenemos un subconjunto manejable.\n",
        "* Mostrando solo el tri√°ngulo inferior de la matriz filtrada, podemos visualizar claramente esas correlaciones y las interrelaciones entre estas variables, sin contaminaci√≥n visual.\n",
        "* Esta t√©cnica facilita la comunicaci√≥n de resultados y el direccionamiento de an√°lisis futuros.\n"
      ],
      "metadata": {
        "id": "rvgG_L9TPIS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "‚úÖ **An√°lisis del nuevo heatmap de correlaci√≥n**\n",
        "\n",
        "> Recordando que el gr√°fico muestra **correlaciones ‚â• 0.2** (o ‚â§ -0.2) con la variable objetivo `Churn_Yes`.\n",
        "\n",
        "---\n",
        "\n",
        "üîç **Correlaci√≥n con `Churn_Yes` (variable objetivo)**\n",
        "\n",
        "| Variable                                 | Correlaci√≥n con Churn\\_Yes | Interpretaci√≥n                                                                                                               |\n",
        "| ---------------------------------------- | -------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `internet.InternetService_Fiber optic`   | **+0.31**                  | Clientes con fibra √≥ptica tienen **mayor probabilidad de churn**. Puede estar relacionado al costo o a la competitividad.    |\n",
        "| `account.PaymentMethod_Electronic check` | **+0.30**                  | Pagos por cheque electr√≥nico est√°n asociados a m√°s churn ‚Äî quiz√°s por perfil de cliente menos fidelizado.                    |\n",
        "| `account.Contract_Two year`              | **-0.30**                  | Contratos de 2 a√±os reducen el churn (clientes m√°s comprometidos o con beneficios)                                           |\n",
        "| `customer.tenure`                        | **-0.35**                  | Cuanto mayor el tiempo como cliente, menor la probabilidad de churn ‚Äî esperado                                               |\n",
        "| `internet.InternetService_No`            | **-0.23**                  | Quienes **no usan internet** tienden a churnar menos ‚Äî posiblemente perfiles m√°s estables (adultos mayores, menos digitales) |\n"
      ],
      "metadata": {
        "id": "zWGtwzHhPgMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç An√°lisis de Multicolinealidad\n"
      ],
      "metadata": {
        "id": "Y3rbLtcWQWJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Usar an√°lisis del Factor de Inflaci√≥n de la Varianza (VIF):**\n",
        "\n",
        "* El VIF ayuda a detectar la presencia de multicolinealidad entre variables independientes.\n",
        "* Generalmente, **VIF > 5** o **VIF > 10** indica que la variable est√° colineal con otras y puede ser eliminada.\n"
      ],
      "metadata": {
        "id": "L8HCXRIZQz28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **¬øCu√°ndo es v√°lido calcular el VIF?**\n",
        "\n",
        "Puedes (y debes) calcular el VIF si:\n",
        "\n",
        "1. **Vas a usar modelos lineales** (ej.: regresi√≥n log√≠stica, regresi√≥n lineal)\n",
        "2. **Quieres interpretar los coeficientes** con claridad (la multicolinealidad distorsiona signos y magnitudes)\n",
        "3. **Quieres garantizar estabilidad en el modelo**\n",
        "\n",
        "---\n",
        "\n",
        "‚ùå ¬øCu√°ndo puedes saltarte el VIF (o postergarlo)?\n",
        "\n",
        "* Si vas a usar **modelos no lineales**, como:\n",
        "\n",
        "  * √Årboles de decisi√≥n\n",
        "  * Random Forest\n",
        "  * XGBoost\n",
        "  * Redes neuronales\n",
        "\n",
        "* Estos modelos **no son sensibles a la multicolinealidad**.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Conclusi√≥n final\n",
        "\n",
        "| Pregunta                                           | Respuesta                                             |\n",
        "| -------------------------------------------------- | ----------------------------------------------------- |\n",
        "| ¬øEl nuevo heatmap muestra multicolinealidad grave? | **No**                                                |\n",
        "| ¬øNecesito calcular el VIF obligatoriamente?        | **No, pero es recomendable si usas modelos lineales** |\n",
        "| ¬øVale la pena como verificaci√≥n extra?             | **S√≠, especialmente si el modelo es interpretativo**  |\n",
        "\n",
        "---\n",
        "\n",
        "Si vas a seguir con regresi√≥n log√≠stica, por ejemplo, **recomendar√≠a calcular el VIF**.\n"
      ],
      "metadata": {
        "id": "Sllz0FRNQbI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importar las bibliotecas necesarias\n"
      ],
      "metadata": {
        "id": "UsHhEOe9Q5XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant"
      ],
      "metadata": {
        "id": "KbLljmljQaqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Seleccionar las variables independientes\n"
      ],
      "metadata": {
        "id": "ToaVFo6_RASA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqu√≠, no incluyas la variable objetivo (ej: Churn\\_Yes) en el c√°lculo del VIF.\n",
        "X = df_encoded.drop(columns=['Churn_Yes'])"
      ],
      "metadata": {
        "id": "YnGOPjSKOmNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. A√±adir constante (intercepto)"
      ],
      "metadata": {
        "id": "23SCyb5XRNvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_const = add_constant(X)"
      ],
      "metadata": {
        "id": "AgDv1S-VRJLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Calcular el VIF"
      ],
      "metadata": {
        "id": "YuMUIa5uRVts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert boolean columns to integers (0 or 1)\n",
        "X_const = X_const.astype(float)\n",
        "\n",
        "# Calcular el VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
        "\n",
        "# Mostrar resultado\n",
        "display(vif_data.sort_values(by='VIF', ascending=False))"
      ],
      "metadata": {
        "id": "xpUlVQKQSdd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Resumen general del an√°lisis VIF**\n",
        "\n",
        "| Rango de VIF       | Interpretaci√≥n                |\n",
        "| ------------------ | ----------------------------- |\n",
        "| VIF ‚âà 1            | Sin multicolinealidad         |\n",
        "| 1 < VIF < 5        | Baja (aceptable)              |\n",
        "| 5 ‚â§ VIF < 10       | Moderada (vigilar)            |\n",
        "| VIF ‚â• 10           | Alta (¬°atenci√≥n!)             |\n",
        "| VIF = ‚àû (infinito) | Multicolinealidad perfecta ‚ö†Ô∏è |\n",
        "\n",
        "---\n",
        "\n",
        "üîç **Principales alertas en tu resultado**\n",
        "\n",
        "‚ùóÔ∏è 1. `phone.PhoneService_Yes` y `phone.MultipleLines_No phone service` ‚Üí VIF = `inf`\n",
        "\n",
        "Estas dos variables **siguen siendo perfectamente colineales entre s√≠** o con otra variable.\n",
        "\n",
        "üîç Causa probable:\n",
        "\n",
        "* Ambas provienen de **la misma variable categ√≥rica original** (`PhoneService`), y el encoding gener√≥ **redundancia**.\n",
        "* Si el cliente **no tiene tel√©fono**, no puede tener m√∫ltiples l√≠neas ‚áí valores 100% ligados.\n",
        "\n",
        "üìå **Soluci√≥n recomendada:**\n",
        "\n",
        "* **Elimina una de estas columnas.** Por ejemplo:\n",
        "\n",
        "  ```python\n",
        "  df.drop(columns=[\"phone.PhoneService_Yes\"], inplace=True)\n",
        "  ```\n",
        "* O rehacer el encoding con `drop_first=True` para simplificar la estructura.\n",
        "\n",
        "---\n",
        "\n",
        "‚ùóÔ∏è 2. `account.Charges.Monthly` ‚Üí VIF = **813.86**\n",
        "\n",
        "* Esto es extremadamente alto.\n",
        "* Esta variable est√° **altamente correlacionada con `account.Charges.Total` y `Total.Day`**.\n",
        "\n",
        "üìå **Soluci√≥n:**\n",
        "\n",
        "* Verifica si `Charges.Monthly`, `Charges.Total` y `Total.Day` contienen **informaci√≥n repetida** (ej.: `Total = Monthly * tenure`).\n",
        "* Si es as√≠, **elimina una o dos de estas columnas** para evitar redundancia.\n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è 3. Otros VIFs altos (moderados a severos)\n",
        "\n",
        "| Variable                          | VIF       | Comentario                                  |\n",
        "| --------------------------------- | --------- | ------------------------------------------- |\n",
        "| `InternetService_Fiber optic`     | 137.9     | Altamente colineal con `InternetService_No` |\n",
        "| `InternetService_No`              | 96.9      | Mismo motivo anterior                       |\n",
        "| `StreamingTV` / `StreamingMovies` | 22.4‚Äì22.5 | Dependen directamente de tener internet     |\n",
        "| `Charges.Total`                   | 10.8      | Relacionado con `Monthly` y `tenure`        |\n",
        "\n",
        "üìå **Soluciones combinadas:**\n",
        "\n",
        "* Mantener **solo una** de las variables entre `InternetService_Fiber optic`, `InternetService_No` o usar `drop_first=True`.\n",
        "* Evaluar si es necesario mantener **todas** las variables derivadas de internet (streaming, seguridad, etc.).\n",
        "* Verificar si `Charges.Total` puede ser **recalculado**, si ya existen `Monthly` y `tenure`.\n"
      ],
      "metadata": {
        "id": "xIy9tEXXTuIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VIF REHECHO**"
      ],
      "metadata": {
        "id": "wnirH97uVPYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ====== Etapa 1: Copiar X original para n√£o modificar o original ======\n",
        "X_filtered = X_const.copy()\n",
        "\n",
        "# ====== Etapa 2: Remover vari√°veis com multicolinearidade perfeita (VIF = inf) ======\n",
        "cols_to_drop = [\n",
        "    \"phone.PhoneService_Yes\",               # Altamente colinear com \"MultipleLines\"\n",
        "    \"phone.MultipleLines_No phone service\"  # Redundante com aus√™ncia de telefone\n",
        "]\n",
        "X_filtered.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# ====== Etapa 3: Remover redund√¢ncias fortes entre vari√°veis num√©ricas ======\n",
        "# Se 'Total.Day' e 'Charges.Total' s√£o derivados de 'Monthly' e 'tenure', mantemos s√≥ um\n",
        "X_filtered.drop(columns=[\"Total.Day\"], inplace=True)\n",
        "\n",
        "# ====== Etapa 4: Remover redund√¢ncia entre dummies da mesma vari√°vel categ√≥rica ======\n",
        "# Se usou get_dummies sem drop_first, voc√™ tem dummies redundantes para InternetService\n",
        "X_filtered.drop(columns=[\"internet.InternetService_No\"], inplace=True)\n",
        "\n",
        "# ====== Etapa 5: Garantir que os dados est√£o em float para o VIF funcionar ======\n",
        "X_filtered = X_filtered.astype(float)\n",
        "\n",
        "# ====== Etapa 6: Recalcular o VIF ======\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X_filtered.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_filtered.values, i) for i in range(X_filtered.shape[1])]\n",
        "\n",
        "# ====== Etapa 7: Exibir os resultados ======\n",
        "display(vif_data.sort_values(by=\"VIF\", ascending=False))"
      ],
      "metadata": {
        "id": "L-ym0ZHmRQ5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ An√°lisis del nuevo VIF\n",
        "\n",
        "| Rango de VIF | Interpretaci√≥n                      |\n",
        "| ------------ | ----------------------------------- |\n",
        "| VIF ‚âà 1      | Sin multicolinealidad (√≥ptimo)      |\n",
        "| 1 < VIF ‚â§ 5  | Baja (aceptable)                    |\n",
        "| 5 < VIF ‚â§ 10 | Moderada (vigilar)                  |\n",
        "| VIF > 10     | Alta (atenci√≥n o posible exclusi√≥n) |\n",
        "\n",
        "---\n",
        "\n",
        "üîç Puntos importantes en tu resultado:\n",
        "\n",
        "| Variable                  | VIF     | Observaciones                                                                               |\n",
        "| ------------------------- | ------- | ------------------------------------------------------------------------------------------- |\n",
        "| `account.Charges.Monthly` | 18.17   | A√∫n con multicolinealidad alta ‚Äî puede estar correlacionada con `Charges.Total` y `tenure`. |\n",
        "| `account.Charges.Total`   | 10.71   | Justo en el l√≠mite ‚Äî posible redundancia con `Monthly` y `tenure`                           |\n",
        "| `const`                   | 35.88   | **Normal** para la constante (`const`) ‚Äî se puede ignorar                                   |\n",
        "| Resto                     | 1.0‚Äì7.5 | Todos con VIF **aceptable u √≥ptimo** ‚úîÔ∏è                                                     |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ Conclusi√≥n pr√°ctica\n",
        "\n",
        "* ‚úÖ **La multicolinealidad cr√≠tica fue resuelta** (sin `inf`, sin redundancia perfecta)\n",
        "* ‚ö†Ô∏è **Solo `Charges.Monthly` y `Charges.Total` siguen colineales entre s√≠** ‚Äî esto ya era esperado\n",
        "\n",
        "---\n",
        "\n",
        "üîß ¬øQu√© puedes hacer ahora?\n",
        "\n",
        "Opci√≥n 1 ‚Äì **Seguir con el modelo como est√°**\n",
        "\n",
        "Si el modelo es robusto (como √°rbol, XGBoost, etc.) y **no necesitas interpretar los coeficientes**, puedes dejarlo as√≠.\n",
        "\n",
        "> La multicolinealidad solo afecta modelos **lineales interpretables**.\n",
        "\n",
        "---\n",
        "\n",
        "Opci√≥n 2 ‚Äì **Eliminar una de las dos variables (`Monthly` o `Total`)**\n",
        "\n",
        "Si quieres **reducir el VIF y simplificar**, puedes mantener **solo una** de ellas:\n",
        "\n",
        "```python\n",
        "# Ejemplo: mantener solo Charges.Monthly\n",
        "X_filtered.drop(columns=[\"account.Charges.Total\"], inplace=True)\n",
        "```\n",
        "\n",
        "Luego solo recalcula el VIF para confirmar que el problema desaparece.\n"
      ],
      "metadata": {
        "id": "dVNk-zABUkPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VIF 2**"
      ],
      "metadata": {
        "id": "8AgB9AWeVUgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# === Eliminar la variable account.Charges.Total ===\n",
        "X_final = X_filtered.drop(columns=[\"account.Charges.Total\"])\n",
        "\n",
        "# === Recalcular el VIF ===\n",
        "vif_data_final = pd.DataFrame()\n",
        "vif_data_final[\"feature\"] = X_final.columns\n",
        "vif_data_final[\"VIF\"] = [variance_inflation_factor(X_final.values, i) for i in range(X_final.shape[1])]\n",
        "\n",
        "# === Mostrar los resultados ordenados ===\n",
        "display(vif_data_final.sort_values(by=\"VIF\", ascending=False))\n"
      ],
      "metadata": {
        "id": "KZLAvpexUKPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Modelos Predictivos\n"
      ],
      "metadata": {
        "id": "D440IbPFYTPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importaci√≥n de las bibliotecas"
      ],
      "metadata": {
        "id": "MBvWHMwMYbLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score"
      ],
      "metadata": {
        "id": "7xe5vU-pYXcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Separar features y target"
      ],
      "metadata": {
        "id": "s1ftcB4OYih_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_encoded.drop(columns=['Churn_Yes'])\n",
        "y = df_encoded['Churn_Yes']"
      ],
      "metadata": {
        "id": "JdrBvnB7YeKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividir entrenamiento y prueba\n"
      ],
      "metadata": {
        "id": "wrMkZtjqYq63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "VWE2V_tTYmao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizar**\n",
        "\n",
        "* Vamos a entrenar Regresi√≥n Log√≠stica, que se beneficia de la normalizaci√≥n. Por eso, normalizamos los datos para este modelo.\n",
        "\n",
        "* En cambio, Random Forest no necesita normalizaci√≥n ‚Äî pero como estamos usando los mismos datos para ambos modelos, los normalizamos para mantener consistencia y simplicidad.\n"
      ],
      "metadata": {
        "id": "CHNK3hoFZQ5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "3AF8H-82ZL5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancear entrenamiento con SMOTE**\n",
        "\n",
        "Porque ya verificamos anteriormente que la proporci√≥n de churn estaba desbalanceada.\n"
      ],
      "metadata": {
        "id": "bcDpmvG6Zqrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "nRh6Ap-hZiwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regresi√≥n Log√≠stica**\n"
      ],
      "metadata": {
        "id": "E0WYxYcZaAXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "```python\n",
        "# Instanciar y entrenar\n",
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "y_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluaci√≥n\n",
        "print(\"Regresi√≥n Log√≠stica\")\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_lr))\n",
        "print(\"Matriz de Confusi√≥n:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "```\n"
      ],
      "metadata": {
        "id": "FnSZCmONZ3M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "a-vegb_iaMyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "```python\n",
        "# Instanciar y entrenar\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_rf = rf.predict(X_test_scaled)\n",
        "y_prob_rf = rf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluaci√≥n\n",
        "print(\"Random Forest\")\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_rf))\n",
        "print(\"Matriz de Confusi√≥n:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "```\n"
      ],
      "metadata": {
        "id": "5_NTbFosaHuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8CJYcsAaSM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}